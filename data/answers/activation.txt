Функция активации определяет, как выход нейрона преобразуется дальше. Часто используемые функции — ReLU, sigmoid и softmax. ReLU обычно применяют в скрытых слоях, sigmoid — для бинарной классификации, softmax — для многоклассовой классификации.
